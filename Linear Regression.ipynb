{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear Regression.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prathwishshetty/Regression/blob/master/Linear%20Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "XblOhFJU2uiV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear Regression in Python  \n",
        "Writing a Python code whose input is a training datset ${(x^1,y^1),...,(x^N,y^N)}$ and its output is the weight vector $\\theta$ in the linear regression model $y = \\theta^t\\phi(x)$, for a given nonlinear mapping $\\phi(Â·)$  \n",
        "  \n",
        "We will first install the dependencies of the program"
      ]
    },
    {
      "metadata": {
        "id": "CmkUFNOa2uiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import array, dot, transpose\n",
        "import pandas as pd\n",
        "from numpy.linalg import inv\n",
        "from scipy import stats\n",
        "import scipy.io as spio\n",
        "import os\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5kAtO8Mq2uia",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creating a function for Closed Form of Linear Regression  \n",
        "We know that the closed form of solving a linear regression in a matrix form is as follows, $\\theta=(x^Tx)^{-1} x^T y$. The same has been implemented below as a python function "
      ]
    },
    {
      "metadata": {
        "id": "zZopSPuN2uia",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_regression_closed(x,y):\n",
        "    x = np.array(x)\n",
        "    ones = np.ones(len(x))\n",
        "    x = np.column_stack((ones,x))\n",
        "    y = np.array(y)    \n",
        "    theta = dot(dot(inv(dot(transpose(x),x)), transpose(x)), y)      \n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kDbGvrwS2uic",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Creating a function for Linear Regression with mini-batch Gradient Descent\n",
        "Function for linear regression in a mini batch gradient descent"
      ]
    },
    {
      "metadata": {
        "id": "53kzMk992uic",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_gradient_descent(x, y, m,alpha,numIterations):\n",
        "    x = np.array(x)\n",
        "    ones = np.ones(len(x))\n",
        "    x = np.column_stack((ones,x))\n",
        "    y = np.array(y)\n",
        "    theta = np.zeros(x.shape[1])\n",
        "    for iter in range(numIterations):\n",
        "        for i in range(0,len(x),m):\n",
        "            x_new=x[i:i+m]\n",
        "            y_new=y[i:i+m]\n",
        "            x_newT=x_new.transpose()\n",
        "            hypothesis = np.dot(x_new, theta)\n",
        "            loss = hypothesis - y_new\n",
        "            J = np.sum(loss ** 2) / (2 * m)  # cost \n",
        "            #print(iter,i,J)\n",
        "            gradient = np.dot(x_newT, loss) / m         \n",
        "            theta = theta - alpha * gradient  # update\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yi8Hzc3E2uie",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a function for ridge regression in closed form\n",
        "Function for ridge regression in closed form"
      ]
    },
    {
      "metadata": {
        "id": "Xa7Iaj0M2uif",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def ridge_regression(x,y,lamb):\n",
        "    x = np.array(x)\n",
        "    ones = np.ones(len(x))\n",
        "    x = np.column_stack((ones,x))\n",
        "    y = np.array(y) \n",
        "    I=np.identity(x.shape[1])\n",
        "    theta = dot(dot(inv(dot(transpose(x),x)+(lamb*I)), transpose(x)), y)\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_DlUu4Sp2uig",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Creating a function for Ridge Regression with batch gradient descent \n",
        "Function for ridge regression in gradient descent"
      ]
    },
    {
      "metadata": {
        "id": "-vToNVwQ2uih",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_ridge_gradient_descent(x, y, m,alpha,numIterations,L):\n",
        "    intercept = np.ones(len(x))\n",
        "    X = np.append(intercept, x)\n",
        "    X = np.reshape(X,(x.shape[0],x.shape[1]+1))\n",
        "    x = stats.zscore(X, axis=0)\n",
        "    y = stats.zscore(y)\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    theta = np.zeros(x.shape[1])\n",
        "    for iter in range(numIterations):\n",
        "        for i in range(0,len(x),m):\n",
        "            x_new=x[i:i+m]\n",
        "            y_new=y[i:i+m]\n",
        "            x_newT=x_new.transpose()\n",
        "            hypothesis = np.dot(x_new, theta)\n",
        "            loss = hypothesis - y_new\n",
        "            J =  J = np.sum(loss ** 2) / (2 * m) # cost \n",
        "            print(iter,i,J)\n",
        "            gradient = np.dot(x_newT, loss) / m         \n",
        "            theta = theta - alpha * (gradient + (L/m)*theta)  # update\n",
        "    return theta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "z1TOApLt2uii",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Test the quality of the models that have been created"
      ]
    },
    {
      "metadata": {
        "id": "p63lzFvo2uij",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_test(x,y,theta_test):\n",
        "    xt = np.array(x)\n",
        "    ones = np.ones(len(xt))\n",
        "    xt = np.column_stack((ones,xt))\n",
        "    yt = np.array(y)\n",
        "    loss = np.sum((np.dot(xt, theta_test) - yt)**2)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b76jAXro2uil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a training dataset by using the random number generator of the `numpy` package"
      ]
    },
    {
      "metadata": {
        "id": "LpsQpktP2uim",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "xTrain=np.random.rand(200,)\n",
        "yTrain=np.random.rand(200,)\n",
        "xTest=np.random.rand(200,)\n",
        "yTest=np.random.rand(200,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fHEco8mR2uio",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Creating a $2^{nd}, 3^{rd}$ and $5^{th}$ degree polynomial of the type:\n",
        "* $y=x+x^2$\n",
        "* $y=x+x^2+x^3$\n",
        "* $y=x+x^2+x^3+x^4+x^5$"
      ]
    },
    {
      "metadata": {
        "id": "Z5YwvRm72uio",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "xTrain_n2 = np.column_stack((xTrain,xTrain**2))\n",
        "xTrain_n3 = np.column_stack((xTrain,xTrain**2,xTrain**3))\n",
        "xTrain_n5 = np.column_stack((xTrain,xTrain**2,xTrain**3,xTrain**4,xTrain**5))\n",
        "xTest_n2 = np.column_stack((xTest,xTest**2))\n",
        "\n",
        "xTest_n3 = np.column_stack((xTest,xTest**2,xTest**3))\n",
        "xTest_n5 = np.column_stack((xTest,xTest**2,xTest**3,xTest**4,xTest**5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "amZIo5Ur2uiq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "fa8b70c9-dead-4a69-da07-ea6716d688e1"
      },
      "cell_type": "code",
      "source": [
        "print(\"2nd degree polynomial\")\n",
        "theta_c_2=linear_regression_closed(xTrain_n2, yTrain)\n",
        "print(theta_c_2)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_2=model_test(xTrain_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
        "print(\"Training Error:\",training_error_c_2)\n",
        "\n",
        "test_error_c_2=model_test(xTest_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
        "print(\"Test Error:\",test_error_c_2)\n",
        "\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"3rd degree polynomial\")\n",
        "theta_c_3=linear_regression_closed(xTrain_n3, yTrain)\n",
        "print(theta_c_3)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_3=model_test(xTrain_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
        "print(\"Training Error:\",training_error_c_3)\n",
        "\n",
        "test_error_c_3=model_test(xTest_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
        "print(\"Test Error:\",test_error_c_3)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"5th degree polynomial\")\n",
        "theta_c_5=linear_regression_closed(xTrain_n5, yTrain)\n",
        "print(theta_c_5)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_5=model_test(xTrain_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
        "print(\"Training Error:\",training_error_c_5)\n",
        "\n",
        "test_error_c_5=model_test(xTest_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
        "print(\"Test Error:\",test_error_c_5)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2nd degree polynomial\n",
            "[ 0.5680825  -0.35216161  0.30010179]\n",
            "Training Error: 17.119520790745174\n",
            "Test Error: 17.29651245246555\n",
            "\n",
            "\n",
            "3rd degree polynomial\n",
            "[ 0.53312554  0.06678217 -0.73162787  0.68065069]\n",
            "Training Error: 17.08687916700004\n",
            "Test Error: 17.090268079181303\n",
            "\n",
            "\n",
            "5th degree polynomial\n",
            "[  0.6476598   -2.47009181  12.62284967 -25.99437832  22.08020129\n",
            "  -6.27041737]\n",
            "Training Error: 16.89635155393961\n",
            "Test Error: 17.58998000476202\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JG5ZeCHh2uiu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "01214fa0-71bb-4452-8993-185ff9f072b5"
      },
      "cell_type": "code",
      "source": [
        "print(\"2nd degree polynomial\")\n",
        "theta_c_2=linear_regression_closed(xTrain_n2, yTrain)\n",
        "print(theta_c_2)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_2=model_test(xTrain_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
        "print(\"Training Error:\",training_error_c_2)\n",
        "\n",
        "test_error_c_2=model_test(xTest_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
        "print(\"Test Error:\",test_error_c_2)\n",
        "\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"3rd degree polynomial\")\n",
        "theta_c_3=linear_regression_closed(xTrain_n3, yTrain)\n",
        "print(theta_c_3)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_3=model_test(xTrain_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
        "print(\"Training Error:\",training_error_c_3)\n",
        "\n",
        "test_error_c_3=model_test(xTest_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
        "print(\"Test Error:\",test_error_c_3)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(\"5th degree polynomial\")\n",
        "theta_c_5=linear_regression_closed(xTrain_n5, yTrain)\n",
        "print(theta_c_5)\n",
        "\n",
        "#Calculate the training error\n",
        "training_error_c_5=model_test(xTrain_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
        "print(\"Training Error:\",training_error_c_5)\n",
        "\n",
        "test_error_c_5=model_test(xTest_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
        "print(\"Test Error:\",test_error_c_5)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2nd degree polynomial\n",
            "[ 0.5680825  -0.35216161  0.30010179]\n",
            "Training Error: 17.119520790745174\n",
            "Test Error: 17.29651245246555\n",
            "\n",
            "\n",
            "3rd degree polynomial\n",
            "[ 0.53312554  0.06678217 -0.73162787  0.68065069]\n",
            "Training Error: 17.08687916700004\n",
            "Test Error: 17.090268079181303\n",
            "\n",
            "\n",
            "5th degree polynomial\n",
            "[  0.6476598   -2.47009181  12.62284967 -25.99437832  22.08020129\n",
            "  -6.27041737]\n",
            "Training Error: 16.89635155393961\n",
            "Test Error: 17.58998000476202\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}