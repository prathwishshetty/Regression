{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression in Python  \n",
    "Writing a Python code whose input is a training datset ${(x^1,y^1),...,(x^N,y^N)}$ and its output is the weight vector $\\theta$ in the linear regression model $y = \\theta^t\\phi(x)$, for a given nonlinear mapping $\\phi(Â·)$  \n",
    "  \n",
    "We will first install the dependencies of the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array, dot, transpose\n",
    "import pandas as pd\n",
    "from numpy.linalg import inv\n",
    "from scipy import stats\n",
    "import scipy.io as spio\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function for Closed Form of Linear Regression  \n",
    "We know that the closed form of solving a linear regression in a matrix form is as follows, $\\theta=(x^Tx)^{-1} x^T y$. The same has been implemented below as a python function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_closed(x,y):\n",
    "    x = np.array(x)\n",
    "    ones = np.ones(len(x))\n",
    "    x = np.column_stack((ones,x))\n",
    "    y = np.array(y)    \n",
    "    theta = dot(dot(inv(dot(transpose(x),x)), transpose(x)), y)      \n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a function for Linear Regression with mini-batch Gradient Descent\n",
    "Function for linear regression in a mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gradient_descent(x, y, m,alpha,numIterations):\n",
    "    x = np.array(x)\n",
    "    ones = np.ones(len(x))\n",
    "    x = np.column_stack((ones,x))\n",
    "    y = np.array(y)\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    for iter in range(numIterations):\n",
    "        for i in range(0,len(x),m):\n",
    "            x_new=x[i:i+m]\n",
    "            y_new=y[i:i+m]\n",
    "            x_newT=x_new.transpose()\n",
    "            hypothesis = np.dot(x_new, theta)\n",
    "            loss = hypothesis - y_new\n",
    "            J = np.sum(loss ** 2) / (2 * m)  # cost \n",
    "            #print(iter,i,J)\n",
    "            gradient = np.dot(x_newT, loss) / m         \n",
    "            theta = theta - alpha * gradient  # update\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function for ridge regression in closed form\n",
    "Function for ridge regression in closed form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression(x,y,lamb):\n",
    "    x = np.array(x)\n",
    "    ones = np.ones(len(x))\n",
    "    x = np.column_stack((ones,x))\n",
    "    y = np.array(y) \n",
    "    I=np.identity(x.shape[1])\n",
    "    theta = dot(dot(inv(dot(transpose(x),x)+(lamb*I)), transpose(x)), y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a function for Ridge Regression with batch gradient descent \n",
    "Function for ridge regression in gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_ridge_gradient_descent(x, y, m,alpha,numIterations,L):\n",
    "    intercept = np.ones(len(x))\n",
    "    X = np.append(intercept, x)\n",
    "    X = np.reshape(X,(x.shape[0],x.shape[1]+1))\n",
    "    x = stats.zscore(X, axis=0)\n",
    "    y = stats.zscore(y)\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    theta = np.zeros(x.shape[1])\n",
    "    for iter in range(numIterations):\n",
    "        for i in range(0,len(x),m):\n",
    "            x_new=x[i:i+m]\n",
    "            y_new=y[i:i+m]\n",
    "            x_newT=x_new.transpose()\n",
    "            hypothesis = np.dot(x_new, theta)\n",
    "            loss = hypothesis - y_new\n",
    "            J =  J = np.sum(loss ** 2) / (2 * m) # cost \n",
    "            print(iter,i,J)\n",
    "            gradient = np.dot(x_newT, loss) / m         \n",
    "            theta = theta - alpha * (gradient + (L/m)*theta)  # update\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the quality of the models that have been created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test(x,y,theta_test):\n",
    "    xt = np.array(x)\n",
    "    ones = np.ones(len(xt))\n",
    "    xt = np.column_stack((ones,xt))\n",
    "    yt = np.array(y)\n",
    "    loss = np.sum((np.dot(xt, theta_test) - yt)**2)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a training dataset by using the random number generator of the `numpy` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "xTrain=np.random.rand(200,)\n",
    "yTrain=np.random.rand(200,)\n",
    "xTest=np.random.rand(200,)\n",
    "yTest=np.random.rand(200,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a $2^{nd}, 3^{rd}$ and $5^{th}$ degree polynomial of the type:\n",
    "* $y=x+x^2$\n",
    "* $y=x+x^2+x^3$\n",
    "* $y=x+x^2+x^3+x^4+x^5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain_n2 = np.column_stack((xTrain,xTrain**2))\n",
    "xTrain_n3 = np.column_stack((xTrain,xTrain**2,xTrain**3))\n",
    "xTrain_n5 = np.column_stack((xTrain,xTrain**2,xTrain**3,xTrain**4,xTrain**5))\n",
    "xTest_n2 = np.column_stack((xTest,xTest**2))\n",
    "\n",
    "xTest_n3 = np.column_stack((xTest,xTest**2,xTest**3))\n",
    "xTest_n5 = np.column_stack((xTest,xTest**2,xTest**3,xTest**4,xTest**5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd degree polynomial\n",
      "[ 0.5680825  -0.35216161  0.30010179]\n",
      "Training Error: 17.119520790745177\n",
      "Test Error: 17.29651245246555\n",
      "\n",
      "\n",
      "3rd degree polynomial\n",
      "[ 0.53312554  0.06678217 -0.73162787  0.68065069]\n",
      "Training Error: 17.08687916700004\n",
      "Test Error: 17.0902680791814\n",
      "\n",
      "\n",
      "5th degree polynomial\n",
      "[  0.6476598   -2.47009181  12.62284967 -25.99437834  22.08020131\n",
      "  -6.27041737]\n",
      "Training Error: 16.896351553939596\n",
      "Test Error: 17.589980004690364\n"
     ]
    }
   ],
   "source": [
    "print(\"2nd degree polynomial\")\n",
    "theta_c_2=linear_regression_closed(xTrain_n2, yTrain)\n",
    "print(theta_c_2)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_2=model_test(xTrain_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
    "print(\"Training Error:\",training_error_c_2)\n",
    "\n",
    "test_error_c_2=model_test(xTest_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
    "print(\"Test Error:\",test_error_c_2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"3rd degree polynomial\")\n",
    "theta_c_3=linear_regression_closed(xTrain_n3, yTrain)\n",
    "print(theta_c_3)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_3=model_test(xTrain_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
    "print(\"Training Error:\",training_error_c_3)\n",
    "\n",
    "test_error_c_3=model_test(xTest_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
    "print(\"Test Error:\",test_error_c_3)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"5th degree polynomial\")\n",
    "theta_c_5=linear_regression_closed(xTrain_n5, yTrain)\n",
    "print(theta_c_5)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_5=model_test(xTrain_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
    "print(\"Training Error:\",training_error_c_5)\n",
    "\n",
    "test_error_c_5=model_test(xTest_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
    "print(\"Test Error:\",test_error_c_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd degree polynomial\n",
      "[ 0.5680825  -0.35216161  0.30010179]\n",
      "Training Error: 17.119520790745177\n",
      "Test Error: 17.29651245246555\n",
      "\n",
      "\n",
      "3rd degree polynomial\n",
      "[ 0.53312554  0.06678217 -0.73162787  0.68065069]\n",
      "Training Error: 17.08687916700004\n",
      "Test Error: 17.0902680791814\n",
      "\n",
      "\n",
      "5th degree polynomial\n",
      "[  0.6476598   -2.47009181  12.62284967 -25.99437834  22.08020131\n",
      "  -6.27041737]\n",
      "Training Error: 16.896351553939596\n",
      "Test Error: 17.589980004690364\n"
     ]
    }
   ],
   "source": [
    "print(\"2nd degree polynomial\")\n",
    "theta_c_2=linear_regression_closed(xTrain_n2, yTrain)\n",
    "print(theta_c_2)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_2=model_test(xTrain_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
    "print(\"Training Error:\",training_error_c_2)\n",
    "\n",
    "test_error_c_2=model_test(xTest_n2,yTrain,linear_regression_closed(xTrain_n2, yTrain))\n",
    "print(\"Test Error:\",test_error_c_2)\n",
    "\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"3rd degree polynomial\")\n",
    "theta_c_3=linear_regression_closed(xTrain_n3, yTrain)\n",
    "print(theta_c_3)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_3=model_test(xTrain_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
    "print(\"Training Error:\",training_error_c_3)\n",
    "\n",
    "test_error_c_3=model_test(xTest_n3,yTrain,linear_regression_closed(xTrain_n3, yTrain))\n",
    "print(\"Test Error:\",test_error_c_3)\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "print(\"5th degree polynomial\")\n",
    "theta_c_5=linear_regression_closed(xTrain_n5, yTrain)\n",
    "print(theta_c_5)\n",
    "\n",
    "#Calculate the training error\n",
    "training_error_c_5=model_test(xTrain_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
    "print(\"Training Error:\",training_error_c_5)\n",
    "\n",
    "test_error_c_5=model_test(xTest_n5,yTrain,linear_regression_closed(xTrain_n5, yTrain))\n",
    "print(\"Test Error:\",test_error_c_5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
